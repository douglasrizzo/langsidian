{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "from langsidian import ChatBot, DocumentBase"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:langchain_community.document_loaders.obsidian:Encountered non-yaml frontmatter\n"
          ]
        }
      ],
      "source": [
        "bot = ChatBot(\n",
        "  docs_path=(Path.home() / \"Documents\" / \"Obsidian\"),\n",
        "  vectorstore_db_path=Path(\"docs/chroma\"),\n",
        "  document_type=DocumentBase.OBSIDIAN,\n",
        "  model_type=\"mistral:7b-instruct\",\n",
        "  embeddings=\"nomic\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "**Question: Give me the equation for the action value function update and explain its terms.**\n",
              "\n",
              "Answer:  The equation for updating the Action Value Function (Q-function) using the Q-learning algorithm with a discount factor (\u03b3) and an learning rate (\u03b1), is given by the following Bellman optimality equation:\n",
              "\n",
              "$$Q_{t+1}(S_t, A_t) \\leftarrow Q_{t}(S_t, A_t) + \\alpha [R_{t+1} + \\gamma \\max\\_{A'} Q_{t}(S_{t+1}, A') - Q_{t}(S_t, A_t)]$$\n",
              "\n",
              "Here's a brief explanation of each term in the equation:\n",
              "\n",
              "* $Q_{t+1}(S_t, A_t)$: The estimated action-value function for the state-action pair $(S_t, A_t)$ at time step $t+1$.\n",
              "* $Q_{t}(S_t, A_t)$: The previously estimated action-value function for the same state-action pair $(S_t, A_t)$ at time step $t$.\n",
              "* $\\alpha$: The learning rate which determines how much weight is given to new information and how much weight is given to old information.\n",
              "* $R_{t+1}$: The reward obtained after taking action $A_t$ in state $S_t$ at time $t$, and moving to the next state $S_{t+1}$.\n",
              "* $\\gamma$: The discount factor which represents the degree of preference for immediate rewards over future rewards. It is a value between 0 and 1, where 0 means that only the present reward matters and 1 means that both present and future rewards matter equally.\n",
              "* $\\max\\_{A'} Q_{t}(S_{t+1}, A')$: The maximum estimated action-value for state $S_{t+1}$ at time step $t+1$, considering all possible actions $A'$ that could be taken from state $S_{t+1}$. This term represents the expected maximum future reward starting from state $S_{t+1}$ and following the optimal policy.\n",
              "\n",
              "By updating the Q-function using this equation, we are iteratively improving our estimate of the optimal action-value function. The goal is to find a policy that maximizes the expected return (or long-term reward) for each state-action pair in the given Markov Decision Process."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "**Question: What is the architecture of the Deep Q-Networks?**\n",
              "\n",
              "Answer:  The Deep Q-Network (DQN) architecture consists of a deep neural network that is used to approximate the Q-function, which maps state-action pairs to expected future rewards. The input to this neural network is a 4 x 84 x 84 tensor representing 84x84 grayscale pixels from an Atari 2600 game screen, and four frames shifted one time step apart. The authors used three convolutional layers in their implementation, followed by two fully connected layers.\n",
              "\n",
              "The first two convolutional layers each had 32 and 64 filters of size 8x8 with a stride of 4x4, respectively. ReLU activation functions were used after each convolutional layer. The third convolutional layer had 64 filters of size 3x3 with a stride of 1x1. The output from this layer was flattened and passed through two fully connected layers with 512 and 512 units respectively, followed by an output layer with the number of outputs equal to the total possible actions in the game (ranging from 4 to 18).\n",
              "\n",
              "The neural network was trained using the Q-learning algorithm with experience replay and target network updates. The authors also used techniques such as double DQN and dueling DQN to improve performance. In double DQN, two separate networks were used for selecting actions and evaluating values, while in dueling DQN, the output layer was split into separate branches for state-value estimation and action-value estimation.\n",
              "\n",
              "Here are the related papers mentioned in the context:\n",
              "\n",
              "* Deep Q-Networks - RNNs & LSTMs: [Deep Recurrent Q-Learning using LSTM for Atari Games](https://arxiv.org/abs/1402.6352)\n",
              "* Dueling Network Architectures for Deep Reinforcement Learning: [arXiv:1511.06581](https://arxiv.org/abs/1511.06581)\n",
              "* Double Q-Learning: [arXiv:1509.06464](https://arxiv.org/abs/1509.06464)"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "**Question: What is the difference between causal language modeling and masked language modeling?**\n",
              "\n",
              "Answer:  Causal language modeling and masked language modeling are two different methods used in pre-training language models. \n",
              "\n",
              "In causal language modeling, the model is trained to predict the next token in a sequence given the context of the previous tokens. The model has access to the first tokens in the sequence and its task is to generate the next token based on the context. This method creates multiple training examples from a single sequence by shifting one token to the right each time, resulting in models that understand the causal relationship between words in a sentence.\n",
              "\n",
              "On the other hand, masked language modeling involves randomly masking tokens at training time and having the model reconstruct the original text by predicting the masked tokens. This method is used in models like BERT, where 15% of the tokens are masked 80% of the time, replaced with a random token 10% of the time, or kept as is 10% of the time. The main objective of masked language modeling is to learn contextual representations of words by forcing the model to predict masked tokens based on their context in a sentence.\n",
              "\n",
              "In summary, causal language modeling focuses on understanding the causal relationship between words in a sequence, while masked language modeling emphasizes learning contextual representations of words."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "**Question: What is zero-shot learning?**\n",
              "\n",
              "Answer:  Zero-shot learning is a concept in machine learning where a model is expected to predict classes it was not explicitly trained on. In the context of Generative AI and Language Model Machines (LLMs), zero-shot learning refers to the capability of an LLM to execute new tasks without being given any explicit examples during training or at inference time. This is different from few-shot learning, where the model learns to perform new tasks using a few examples provided along with the task definition.\n",
              "\n",
              "Zero-shot learning puts more emphasis on the model's ability to generalize and understand the underlying patterns or features of the data, making it particularly relevant when dealing with open-domain text classification or natural language processing tasks where new classes may emerge continuously. However, due to its reliance on understanding the context, zero-shot learning models might struggle with domain-specific tasks or lack robustness against outliers and noisy data.\n",
              "\n",
              "Zero-shot learning can be contrasted with one-shot learning, where a model is asked to perform some task it was not explicitly trained for but given only one example of that task. However, in the context of LLMs, one-shot learning usually implies few-shot learning as it's more practical and beneficial to provide the model with a few examples rather than just one.\n",
              "\n",
              "Regarding your question about \"Why no importance sampling in Q-learning?\", Q-learning is a type of Reinforcement Learning (RL) algorithm, which relies on exploring new actions in an environment to learn the optimal policy. Importance sampling is a technique used in RL for efficient Monte Carlo estimation of state-action values by assigning weights to the importance of each sampled trajectory based on its probability distribution. However, it's not typically used in Q-learning due to its computational complexity and the existence of alternative methods like epsilon-greedy or UCB exploration strategies that provide more straightforward ways to explore the state-action space efficiently while learning the optimal policy."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "**Question: Explain to me the concept of bucketing in RNNs.**\n",
              "\n",
              "Answer:  Bucketing is a technique used in Recurrent Neural Networks (RNNs) for batching sequences while minimizing padding. The main goal is to keep sequences within a single batch as close in length as possible, which improves the training efficiency and reduces the amount of padding required.\n",
              "\n",
              "The process of implementing bucketing involves dividing data into `m` buckets, where each bucket represents a specific range of sequence lengths. During training, all sequences of a batch are sampled from a single bucket to ensure they have similar lengths. Afterward, these sequences are padded to match the length of the longest sequence in the batch, allowing them to be processed together within an RNN.\n",
              "\n",
              "This approach is beneficial because it minimizes padding and ensures that computational resources are effectively utilized during training. It also results in a more homogeneous set of sequences within a single batch, improving both model stability and convergence rates.\n",
              "\n",
              "However, there are alternative architectures such as bidirectional RNNs (Bi-RNNs) which can process sequences from both ends to capture dependencies in both directions, even when sequence lengths vary significantly. In such cases, padding might not be necessary or detrimental to the model performance. Nonetheless, bucketing remains an essential technique for efficiently processing batches of sequences with similar lengths in standard RNN architectures.\n",
              "\n",
              "For more information and insights on this topic, you can refer to:\n",
              "\n",
              "* [PyTorch documentation](https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pack_padded_sequence.html)\n",
              "* [Coursera: Sequence Models in NLP](https://www.coursera.org/learn/sequence-models-in-nlp/supplement/t5L3H/gated-recurrent-units)\n",
              "* [Rashmi Margani's Medium post](https://rashmi-margani.medium.com/how-to-speed-up-the-training-of-the-sequence-model-using-bucketing-techniques-9e302b0fd976)\n",
              "* [Cho et al., 2014 - Learning Phrase Representations using RNN Encoder\u2013Decoder for Statistical Machine Translation](https://arxiv.org/abs/1406.1078)"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "**Question: What is a named entity?**\n",
              "\n",
              "Answer:  A named entity is a real-world object that can be denoted with a proper name. This can include objects that have a physical existence such as a person, location, organization, or product, as well as abstract concepts like events and natural phenomena. Named entities are important in Natural Language Processing (NLP) as they help in understanding the context of text data.\n",
              "\n",
              "Named Entity Recognition (NER) is a task in NLP that involves automatically identifying and extracting named entities from unstructured text. This can be achieved by using predefined classes such as \"geo\" for geographical entities, \"org\" for organizations, \"per\" for persons, \"gpe\" for geopolitical entities, \"tim\" for time indicators, \"art\" for artifacts, \"eve\" for events, and \"nat\" for natural phenomena. An additional class, \"O,\" is used for filler words or tokens that are not named entities.\n",
              "\n",
              "Named entities have various applications in different fields such as search engine efficiency, where they can be scanned from websites and stored to match user queries, and recommendation systems, where named entities are extracted from a user's search history to identify potential interests. (Refer to the examples in the provided image and sentence.)"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "questions = [\n",
        "  \"Give me the equation for the action value function update and explain its terms.\",\n",
        "  \"What is the architecture of the Deep Q-Networks?\",\n",
        "  \"What is the difference between causal language modeling and masked language modeling?\",\n",
        "  \"What is zero-shot learning?\",\n",
        "  \"Explain to me the concept of bucketing in RNNs.\",\n",
        "  \"What is a named entity?\",\n",
        "]\n",
        "\n",
        "for q in questions:\n",
        "  result = bot.answer(q)\n",
        "  display(Markdown(f\"**Question: {q}**\\n\\nAnswer: {result}\"))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "langsidian",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
